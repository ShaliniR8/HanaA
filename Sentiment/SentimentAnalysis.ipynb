{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(r'C:\\Users\\KIIT\\Downloads\\Train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\KIIT\\Downloads\\Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_indices, test_indices =  40000 5000\n",
      "total.shape =  (45000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_indices = train.shape[0]\n",
    "test_indices = test.shape[0]\n",
    "print(\"train_indices, test_indices = \", train_indices, test_indices )\n",
    "total = pd.concat([train, test], axis=0, ignore_index = True)\n",
    "print(\"total.shape = \",total.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING** \\\n",
    "1.Removing special characters\\\n",
    "2.Tokenize and remove stopwords\\\n",
    "3.Stemming\\\n",
    "4.Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def preprocess(total):\n",
    "\n",
    "\n",
    "    def preprocessing(text):    \n",
    "        text = re.sub('<[^>]*>','',text)\n",
    "        text = re.sub('[\\W+^]',' ',text) \n",
    "        return text\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    def my_swremove(text):\n",
    "        text=word_tokenize(text)\n",
    "        new_text =[ words.lower() for words in text if words not in stop]\n",
    "        return new_text\n",
    "\n",
    "    port =  PorterStemmer()\n",
    "    def my_stemmer(text):\n",
    "\n",
    "        stemmed = [ port.stem(words) for words in text]\n",
    "        return stemmed\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "    def my_lemmatizer(text):\n",
    "        lem_text = [ lem.lemmatize(words) for words in text ]\n",
    "        return ' '.join(lem_text)\n",
    "    \n",
    "    total['text'] = total['text'].apply(preprocessing)\n",
    "    total['text'] = total['text'].apply(my_swremove)\n",
    "    total['text'] = total['text'].apply(my_stemmer)\n",
    "    total['text'] = total['text'].apply(my_lemmatizer)\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF_IDF VECTOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "def vectorize(text):\n",
    "    tfid = TfidfVectorizer( smooth_idf = True, use_idf = True, preprocessor = None)\n",
    "    X = tfid.fit_transform(text)\n",
    "    saved_tfidf = open('saved_tfidf.sav', 'wb')\n",
    "    pickle.dump(tfid , saved_tfidf)\n",
    "    saved_tfidf.close()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPLYING ALL FUNCTIONS TO DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i grew b 1965 watch love thunderbird all mate school watch we play thunderbird school lunch school we want virgil scott no one want alan count 5 becam art form i took child see movi hope would get glimps i love child how bitterli disappoint the high point snappi theme tune not could compar origin score thunderbird thank earli saturday morn one televis channel still play rerun seri gerri anderson wife creat jonatha frake hand director chair version complet hopeless a wast film utter rubbish a cgi remak may accept replac marionett homo sapien subsp sapien huge error judgment'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = preprocess(total)\n",
    "total['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorize(total['text'])\n",
    "y = total['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN TEST SPLIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size= 1/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=4, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=300, multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                     random_state=None, refit=True, scoring='accuracy',\n",
       "                     solver='lbfgs', tol=0.0001, verbose=3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr = LogisticRegressionCV(\n",
    "    cv = 4,\n",
    "    scoring = 'accuracy',\n",
    "    verbose = 3,\n",
    "    max_iter = 300,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = open( 'saved_model.sav', 'wb')\n",
    "pickle.dump( lr, saved_model )\n",
    "saved_model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score =  89.53%\n"
     ]
    }
   ],
   "source": [
    "print(\"score = \", str(round(lr.score( X_test, y_test )*100,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = pickle.load(open( 'saved_model.sav' , 'rb' ))\n",
    "tfidf = pickle.load(open( 'saved_tfidf.sav' , 'rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\" It was really good actually\", \"Truth to be told, it was bad.\" ]\n",
    "test_df = pd.DataFrame(s)\n",
    "test_df.columns = ['text']\n",
    "test_df = preprocess(test_df)\n",
    "vec_test = tfidf.transform(test_df['text'])\n",
    "list(model.predict ( vec_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
